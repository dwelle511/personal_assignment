# 🧠 Hugging Face Model Usage Guide

[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow?style=flat-square)](https://huggingface.co/)
[![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=flat-square&logo=pytorch&logoColor=white)](https://pytorch.org/)

이 문서는 Hugging Face `transformers` 라이브러리를 사용하여 모델을 로드하고, 추론(Inference)하고, 로컬에 저장하는 **핵심 활용 패턴**을 다룹니다.

---

## 📦 1. 기본 개념 (Concepts)

Hugging Face 모델 활용은 보통 3단계 흐름을 가집니다.

1.  **Tokenizer (전처리):** 텍스트를 모델이 이해하는 숫자(Tensor)로 변환.
2.  **Model (추론):** 숫자를 입력받아 결과값(Logits) 출력.
3.  **Post-processing (후처리):** 결과값을 사람이 이해할 수 있는 형태(확률, 텍스트 등)로 변환.

---

## 🚀 2. 텍스트 생성 (Text Generation)
`AutoTokenizer`와 `AutoModelForCausalLM`을 사용한 GPT 계열 모델 활용법입니다.

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# 1. 모델과 토크나이저 로드 (GPT-2 예시)
model_id = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)

# 2. 입력 데이터 전처리 (Tokenization)
text = "The future of AI is"
inputs = tokenizer(text, return_tensors="pt") # PyTorch 텐서로 반환

# 3. 모델 추론 (Generate)
# max_new_tokens: 생성할 최대 토큰 수
outputs = model.generate(**inputs, max_new_tokens=30)

# 4. 결과 디코딩 (Decoding)
result = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(f"Input: {text}")
print(f"Output: {result}")
```

## ⚡ 3. GPU 가속 사용 (Using CUDA)

모델을 GPU로 옮겨 추론 속도를 높이는 방법입니다.
```
import torch
from transformers import pipeline

# GPU 사용 가능 여부 확인
device = 0 if torch.cuda.is_available() else -1
print(f"Using device: {'GPU' if device == 0 else 'CPU'}")

# 파이프라인에 device 파라미터 전달
# device=0 (GPU), device=-1 (CPU)
classifier = pipeline("sentiment-analysis", model="bert-base-multilingual-uncased-sentiment", device=device)

results = classifier(["이 영화 정말 재밌어요!", "This movie was terrible."])
print(results)
```
## 💾 4. 로컬 저장 및 불러오기 (Save & Load)

인터넷 연결이 없는 환경을 위해 모델을 로컬 디렉토리에 저장하고 불러옵니다.

```
from transformers import AutoTokenizer, AutoModel

model_id = "bert-base-uncased"

# --- [저장 단계] ---
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModel.from_pretrained(model_id)

# 로컬 폴더('./my_local_bert')에 저장
tokenizer.save_pretrained("./my_local_bert")
model.save_pretrained("./my_local_bert")

# --- [로드 단계] ---
# 인터넷 없이 로컬 경로에서 바로 로드 가능
local_tokenizer = AutoTokenizer.from_pretrained("./my_local_bert")
local_model = AutoModel.from_pretrained("./my_local_bert")

print("Local model loaded successfully!")
```

## 🛠️ 5. 커스텀 모델 활용 (Features Extraction)

분류나 생성이 아닌, 문장의 임베딩(Embedding) 벡터를 추출할 때 사용합니다.
```
from transformers import AutoTokenizer, AutoModel
import torch

# BERT 모델 로드
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModel.from_pretrained("bert-base-uncased")

inputs = tokenizer("Hello Hugging Face!", return_tensors="pt")

# 추론 (No Gradient calculation for inference)
with torch.no_grad():
    outputs = model(**inputs)

# last_hidden_state: (Batch_Size, Sequence_Length, Hidden_Size)
embeddings = outputs.last_hidden_state

print(f"Embedding Shape: {embeddings.shape}")
# 예: torch.Size([1, 6, 768]) -> 6개 토큰 각각에 대한 768차원 벡터
```
